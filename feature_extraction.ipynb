{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import os\n",
    "import radiomics\n",
    "from radiomics import featureextractor\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from copy import copy\n",
    "import pydicom\n",
    "from collections import defaultdict\n",
    "import nibabel as nib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the mri & mask dict\n",
    "Contains both UMCG and radboud path strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "with open(\"20_mask_dict.pickle\", \"rb\") as pkl_handle:\n",
    "    output = pickle.load(pkl_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_mask_list_copy = copy(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_mask_list_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation experiment\n",
    "using parameter files and a non-normalized ADC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all possible setting files\n",
    "setting_list = []\n",
    "\n",
    "for dir_path, sub_dir, files in os.walk(os.path.normpath(\"./Radiomics_settings\"), topdown=True):\n",
    "    for file_ in files:\n",
    "        if file_.startswith(\"int\"):\n",
    "            full_path = os.path.join(dir_path,file_)\n",
    "            setting_list.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sepearate lists for each specific sequence or deviating setting\n",
    "t2_list_3d = [x for x in setting_list if '_t2_' in x and '2D' not in x and 'ANISO' not in x]\n",
    "t2_list_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sepearate lists for each specific sequence or deviating setting\n",
    "t2_list_3d_aniso = [x for x in setting_list if '_t2_' in x and '2D' not in x and 'ANISO' in x]\n",
    "t2_list_3d_aniso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sepearate lists for each specific sequence or deviating setting\n",
    "t2_list_2d = [x for x in setting_list if '_t2_' in x and '2D' in x]\n",
    "t2_list_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sepearate lists for each specific sequence or deviating setting\n",
    "dwi_list_3d = [x for x in setting_list if '_dwi_' in x and '2D' not in x and 'ANISO' not in x]\n",
    "dwi_list_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sepearate lists for each specific sequence or deviating setting\n",
    "dwi_list_3d_aniso = [x for x in setting_list if '_dwi_' in x and '2D' not in x and 'ANISO' in x]\n",
    "dwi_list_3d_aniso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sepearate lists for each specific sequence or deviating setting\n",
    "dwi_list_2d = [x for x in setting_list if '_dwi_' in x and '2D' in x]\n",
    "dwi_list_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2-Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radiomics_calc_t2(file_mask_combo_dict, params_file):\n",
    "        \"\"\" Function that calculates radiomics features with a given mri/mask dict supplied        \n",
    "        \"\"\"\n",
    "        # Due to more than one finding in some MR studies we need to keep check of already completed feature extractions\n",
    "        dataframe_list = []\n",
    "        problem_list = []\n",
    "               \n",
    "        # Dictionary keys are used as identifiers and can also be used to find the correct masks\n",
    "        for keys in file_mask_combo_dict.keys():\n",
    "            print(keys)\n",
    "            if \"mark\" in keys:\n",
    "                continue\n",
    "            for mr_and_mask in file_mask_combo_dict[keys]:\n",
    "                mr_file_path = mr_and_mask[0]\n",
    "                mask_file_path = mr_and_mask[1] \n",
    "                \n",
    "                # Take the selected sequence only\n",
    "                if 't2' in mr_file_path.lower():\n",
    "\n",
    "                    #Take detailed description for dataframe\n",
    "                    detail_id1 = mask_file_path.rfind(\"\\\\\")\n",
    "                    id_string = mask_file_path[detail_id1+1:]                \n",
    "\n",
    "                    # Initiate extractor with hardcoded settings\n",
    "                    extractor = radiomics.featureextractor.RadiomicsFeatureExtractor(params_file)\n",
    "\n",
    "                    # Remove the unwanted general information that is automatically extracted\n",
    "                    keys_remove = ('general_info_BoundingBox', 'general_info_EnabledImageTypes','general_info_GeneralSettings','general_info_ImageHash', 'general_info_ImageSpacing', 'general_info_MaskHash', 'general_info_NumpyVersion', 'general_info_PyWaveletVersion', 'general_info_SimpleITKVersion','general_info_Version','general_info_VolumeNum','general_info_VoxelNum',\n",
    "                                      'diagnostics_Versions_PyRadiomics', 'diagnostics_Versions_Numpy', 'diagnostics_Versions_SimpleITK', 'diagnostics_Versions_PyWavelet', 'diagnostics_Versions_Python', 'diagnostics_Configuration_Settings', 'diagnostics_Configuration_EnabledImageTypes', 'diagnostics_Image-original_Hash', 'diagnostics_Image-original_Spacing', 'diagnostics_Image-original_Size', 'diagnostics_Image-original_Mean', 'diagnostics_Image-original_Minimum', 'diagnostics_Image-original_Maximum', 'diagnostics_Mask-original_Hash', 'diagnostics_Mask-original_Spacing', 'diagnostics_Mask-original_Size', 'diagnostics_Mask-original_BoundingBox', 'diagnostics_Mask-original_VoxelNum', 'diagnostics_Mask-original_VolumeNum', 'diagnostics_Mask-original_CenterOfMassIndex', 'diagnostics_Mask-original_CenterOfMass', 'diagnostics_Image-interpolated_Spacing', 'diagnostics_Image-interpolated_Size', 'diagnostics_Image-interpolated_Mean', 'diagnostics_Image-interpolated_Minimum', 'diagnostics_Image-interpolated_Maximum', 'diagnostics_Mask-interpolated_Spacing', 'diagnostics_Mask-interpolated_Size', 'diagnostics_Mask-interpolated_BoundingBox', 'diagnostics_Mask-interpolated_VoxelNum', 'diagnostics_Mask-interpolated_VolumeNum', 'diagnostics_Mask-interpolated_CenterOfMassIndex', 'diagnostics_Mask-interpolated_CenterOfMass', 'diagnostics_Mask-interpolated_Mean', 'diagnostics_Mask-interpolated_Minimum', 'diagnostics_Mask-interpolated_Maximum',\n",
    "                                        'diagnostics_Mask-corrected_Spacing', 'diagnostics_Mask-corrected_Size','diagnostics_Mask-corrected_BoundingBox', 'diagnostics_Mask-corrected_VoxelNum', 'diagnostics_Mask-corrected_VolumeNum', 'diagnostics_Mask-corrected_CenterOfMassIndex', 'diagnostics_Mask-corrected_CenterOfMass', 'diagnostics_Mask-corrected_Mean', 'diagnostics_Mask-corrected_Minimum', 'diagnostics_Mask-corrected_Maximum')\n",
    "                    try:\n",
    "                        # Initiate the feature extraction\n",
    "                        feature_vector = extractor.execute(mr_file_path, mask_file_path) \n",
    "\n",
    "                        for k in keys_remove:\n",
    "                            feature_vector.pop(k, None)\n",
    "\n",
    "                        # Create Temporary dataframes\n",
    "                        temporary_df = pd.DataFrame(feature_vector, columns=feature_vector.keys(), index=[keys])\n",
    "                        temporary_df['Details'] = id_string\n",
    "                        temporary_df['Anon_ID'] = keys\n",
    "                        temporary_df['file_name'] = mr_file_path\n",
    "                        dataframe_list.append(temporary_df)            \n",
    "                    except:\n",
    "                        problem_list.append((mr_file_path, mask_file_path))\n",
    "                                   \n",
    "               \n",
    "        # Create one big dataframe based on the list of dataframes\n",
    "        df_result = pd.concat(dataframe_list)\n",
    "               \n",
    "        return(df_result, problem_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for current_set in  [t2_list_3d, t2_list_3d_aniso,t2_list_2d]:\n",
    "    for t2_settings in current_set:\n",
    "        # remove .yml from string\n",
    "        index = t2_settings.rfind(\"\\\\\")\n",
    "        yml_index = t2_settings.rfind('.yml')\n",
    "        current_settings = t2_settings[index+1:yml_index]\n",
    "        print(current_settings)\n",
    "\n",
    "        # run feature calculation with current parameter file\n",
    "        result_df_t2, error_check_list_t2 = radiomics_calc_t2(mri_mask_list_copy, t2_settings)\n",
    "\n",
    "        file_name_t2 = \"RADIOMICS_DATA_20mmDLM_\" + current_settings\n",
    "        print(file_name_t2)\n",
    "        result_df_t2.to_csv(file_name_t2 + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADC & DWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radiomics_calc_dwi3(file_mask_combo_dict, params_file_dwi):\n",
    "        \"\"\" Function that calculates radiomics features with a given mri/mask dict supplied        \n",
    "        \"\"\"\n",
    "        # Due to more than one finding in some MR studies we need to keep check of already completed feature extractions\n",
    "        dataframe_list = []\n",
    "        problem_list = []\n",
    "               \n",
    "        # Dictionary keys are used as identifiers and can also be used to find the correct masks\n",
    "        for keys in file_mask_combo_dict.keys():\n",
    "            print(keys)\n",
    "            if \"mark\" in keys:\n",
    "                continue\n",
    "            for mr_and_mask in file_mask_combo_dict[keys]:\n",
    "                mr_file_path = mr_and_mask[0]\n",
    "                mask_file_path = mr_and_mask[1] \n",
    "                \n",
    "                #Take detailed description for dataframe\n",
    "                detail_id1 = mask_file_path.rfind(\"\\\\\")\n",
    "                id_string = mask_file_path[detail_id1+1:]                \n",
    "\n",
    "                # Initiate extractor with hardcoded settings\n",
    "                extractor = radiomics.featureextractor.RadiomicsFeatureExtractor(params_file_dwi)\n",
    "\n",
    "                # Remove the unwanted general information that is automatically extracted\n",
    "                keys_remove = ('general_info_BoundingBox', 'general_info_EnabledImageTypes','general_info_GeneralSettings','general_info_ImageHash', 'general_info_ImageSpacing', 'general_info_MaskHash', 'general_info_NumpyVersion', 'general_info_PyWaveletVersion', 'general_info_SimpleITKVersion','general_info_Version','general_info_VolumeNum','general_info_VoxelNum',\n",
    "                                  'diagnostics_Versions_PyRadiomics', 'diagnostics_Versions_Numpy', 'diagnostics_Versions_SimpleITK', 'diagnostics_Versions_PyWavelet', 'diagnostics_Versions_Python', 'diagnostics_Configuration_Settings', 'diagnostics_Configuration_EnabledImageTypes', 'diagnostics_Image-original_Hash', 'diagnostics_Image-original_Spacing', 'diagnostics_Image-original_Size', 'diagnostics_Image-original_Mean', 'diagnostics_Image-original_Minimum', 'diagnostics_Image-original_Maximum', 'diagnostics_Mask-original_Hash', 'diagnostics_Mask-original_Spacing', 'diagnostics_Mask-original_Size', 'diagnostics_Mask-original_BoundingBox', 'diagnostics_Mask-original_VoxelNum', 'diagnostics_Mask-original_VolumeNum', 'diagnostics_Mask-original_CenterOfMassIndex', 'diagnostics_Mask-original_CenterOfMass', 'diagnostics_Image-interpolated_Spacing', 'diagnostics_Image-interpolated_Size', 'diagnostics_Image-interpolated_Mean', 'diagnostics_Image-interpolated_Minimum', 'diagnostics_Image-interpolated_Maximum', 'diagnostics_Mask-interpolated_Spacing', 'diagnostics_Mask-interpolated_Size', 'diagnostics_Mask-interpolated_BoundingBox', 'diagnostics_Mask-interpolated_VoxelNum', 'diagnostics_Mask-interpolated_VolumeNum', 'diagnostics_Mask-interpolated_CenterOfMassIndex', 'diagnostics_Mask-interpolated_CenterOfMass', 'diagnostics_Mask-interpolated_Mean', 'diagnostics_Mask-interpolated_Minimum', 'diagnostics_Mask-interpolated_Maximum',\n",
    "                                    'diagnostics_Mask-corrected_Spacing', 'diagnostics_Mask-corrected_Size','diagnostics_Mask-corrected_BoundingBox', 'diagnostics_Mask-corrected_VoxelNum', 'diagnostics_Mask-corrected_VolumeNum', 'diagnostics_Mask-corrected_CenterOfMassIndex', 'diagnostics_Mask-corrected_CenterOfMass', 'diagnostics_Mask-corrected_Mean', 'diagnostics_Mask-corrected_Minimum', 'diagnostics_Mask-corrected_Maximum')\n",
    "\n",
    "                try:\n",
    "                    # Initiate the feature extraction\n",
    "                    feature_vector = extractor.execute(mr_file_path, mask_file_path) \n",
    "\n",
    "                    for k in keys_remove:\n",
    "                        feature_vector.pop(k, None)\n",
    "\n",
    "                    # Create Temporary dataframes\n",
    "                    temporary_df = pd.DataFrame(feature_vector, columns=feature_vector.keys(), index=[keys])\n",
    "                    temporary_df['Details'] = id_string\n",
    "                    temporary_df['Anon_ID'] = keys\n",
    "                    temporary_df['file_name'] = mr_file_path\n",
    "                    dataframe_list.append(temporary_df)            \n",
    "                except:\n",
    "                    problem_list.append((mr_file_path, mask_file_path))                            \n",
    "                                   \n",
    "               \n",
    "        # Create one big dataframe based on the list of dataframes\n",
    "        df_result = pd.concat(dataframe_list)\n",
    "               \n",
    "        return(df_result, problem_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dwi datasets\n",
    "for current_set in  [dwi_list_3d, dwi_list_3d_aniso, dwi_list_2d]:\n",
    "    for dwi_settings in current_set:\n",
    "        # remove .yml from string\n",
    "        index_adc = dwi_settings.rfind(\"\\\\\")\n",
    "        yml_index = dwi_settings.rfind('.yml')\n",
    "        current_settings_adc = dwi_settings[index_adc+1:yml_index]\n",
    "        print(current_settings_adc)\n",
    "\n",
    "        # run feature calculation with current parameter file\n",
    "        result_df_dwi, error_check_list_dwi = radiomics_calc_dwi3(mri_mask_list_copy, dwi_settings)\n",
    "\n",
    "        file_name_adc = \"RADIOMICS_DATA_20mmDLM_\" + current_settings_adc\n",
    "        print(file_name_adc)\n",
    "        result_df_dwi.to_csv(file_name_adc + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Sequences\n",
    "## For the comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radiomics_calc(file_mask_combo_dict, params_file):\n",
    "        \"\"\" Function that calculates radiomics features with a given mri/mask dict supplied        \n",
    "        \"\"\"\n",
    "        # Due to more than one finding in some MR studies we need to keep check of already completed feature extractions\n",
    "        dataframe_list = []\n",
    "        problem_list = []\n",
    "               \n",
    "        # Dictionary keys are used as identifiers and can also be used to find the correct masks\n",
    "        for keys in file_mask_combo_dict.keys():\n",
    "            print(keys)\n",
    "            if \"mark\" in keys:\n",
    "                continue\n",
    "            for mr_and_mask in file_mask_combo_dict[keys]:\n",
    "                mr_file_path = mr_and_mask[0]\n",
    "                mask_file_path = mr_and_mask[1] \n",
    "                \n",
    "                #Take detailed description for dataframe\n",
    "                detail_id1 = mask_file_path.rfind(\"\\\\\")\n",
    "                id_string = mask_file_path[detail_id1+1:]                \n",
    "\n",
    "                # Initiate extractor with hardcoded settings\n",
    "                extractor = radiomics.featureextractor.RadiomicsFeatureExtractor(params_file)\n",
    "\n",
    "                # Remove the unwanted general information that is automatically extracted\n",
    "                keys_remove = ('general_info_BoundingBox', 'general_info_EnabledImageTypes','general_info_GeneralSettings','general_info_ImageHash', 'general_info_ImageSpacing', 'general_info_MaskHash', 'general_info_NumpyVersion', 'general_info_PyWaveletVersion', 'general_info_SimpleITKVersion','general_info_Version','general_info_VolumeNum','general_info_VoxelNum',\n",
    "                                  'diagnostics_Versions_PyRadiomics', 'diagnostics_Versions_Numpy', 'diagnostics_Versions_SimpleITK', 'diagnostics_Versions_PyWavelet', 'diagnostics_Versions_Python', 'diagnostics_Configuration_Settings', 'diagnostics_Configuration_EnabledImageTypes', 'diagnostics_Image-original_Hash', 'diagnostics_Image-original_Spacing', 'diagnostics_Image-original_Size', 'diagnostics_Image-original_Mean', 'diagnostics_Image-original_Minimum', 'diagnostics_Image-original_Maximum', 'diagnostics_Mask-original_Hash', 'diagnostics_Mask-original_Spacing', 'diagnostics_Mask-original_Size', 'diagnostics_Mask-original_BoundingBox', 'diagnostics_Mask-original_VoxelNum', 'diagnostics_Mask-original_VolumeNum', 'diagnostics_Mask-original_CenterOfMassIndex', 'diagnostics_Mask-original_CenterOfMass', 'diagnostics_Image-interpolated_Spacing', 'diagnostics_Image-interpolated_Size', 'diagnostics_Image-interpolated_Mean', 'diagnostics_Image-interpolated_Minimum', 'diagnostics_Image-interpolated_Maximum', 'diagnostics_Mask-interpolated_Spacing', 'diagnostics_Mask-interpolated_Size', 'diagnostics_Mask-interpolated_BoundingBox', 'diagnostics_Mask-interpolated_VoxelNum', 'diagnostics_Mask-interpolated_VolumeNum', 'diagnostics_Mask-interpolated_CenterOfMassIndex', 'diagnostics_Mask-interpolated_CenterOfMass', 'diagnostics_Mask-interpolated_Mean', 'diagnostics_Mask-interpolated_Minimum', 'diagnostics_Mask-interpolated_Maximum',\n",
    "                                    'diagnostics_Mask-corrected_Spacing', 'diagnostics_Mask-corrected_Size','diagnostics_Mask-corrected_BoundingBox', 'diagnostics_Mask-corrected_VoxelNum', 'diagnostics_Mask-corrected_VolumeNum', 'diagnostics_Mask-corrected_CenterOfMassIndex', 'diagnostics_Mask-corrected_CenterOfMass', 'diagnostics_Mask-corrected_Mean', 'diagnostics_Mask-corrected_Minimum', 'diagnostics_Mask-corrected_Maximum')\n",
    "                try:\n",
    "                    # Initiate the feature extraction\n",
    "                    feature_vector = extractor.execute(mr_file_path, mask_file_path) \n",
    "\n",
    "                    for k in keys_remove:\n",
    "                        feature_vector.pop(k, None)\n",
    "\n",
    "                    # Create Temporary dataframes\n",
    "                    temporary_df = pd.DataFrame(feature_vector, columns=feature_vector.keys(), index=[keys])\n",
    "                    temporary_df['Details'] = id_string\n",
    "                    temporary_df['Anon_ID'] = keys\n",
    "                    temporary_df['file_name'] = mr_file_path\n",
    "                    dataframe_list.append(temporary_df)            \n",
    "                except:\n",
    "                    problem_list.append((mr_file_path, mask_file_path))\n",
    "\n",
    "               \n",
    "        # Create one big dataframe based on the list of dataframes\n",
    "        df_result = pd.concat(dataframe_list)\n",
    "               \n",
    "        return(df_result, problem_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_set = [\"./Radiomics_settings/2D_noresampling.yml\",\n",
    "          \"./Radiomics_settings/3D_noresampling.yml\"]\n",
    "\n",
    "for i in gen_set:\n",
    "    # Run feature calculation\n",
    "    result_df, error_check_list = radiomics_calc(mri_mask_list_copy, i)\n",
    "\n",
    "    # Define filename and save feature dataframe to csv\n",
    "    name = i[i.rfind(\"/\")+1:]\n",
    "    name_final = name.replace(\".yml\",\"\")\n",
    "    file_name = \"[Radiomics_data]_20mmDLM_\" + name_final\n",
    "    \n",
    "    result_df.to_csv(file_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open general 2D radiomics settings\n",
    "general_settings = \"./Radiomics_settings/2D_noresampling_comparison.yml\"\n",
    "\n",
    "# Run feature calculation\n",
    "result_df, error_check_list = radiomics_calc(mri_mask_list_copy, general_settings)\n",
    "\n",
    "# Define filename and save feature dataframe to csv\n",
    "file_name = \"[Radiomics_data]_18mmDLM_2D_noresampling_comparison\" \n",
    "result_df.to_csv(file_name + '.csv')\n",
    "\n",
    "# For easy readability also create excel file\n",
    "writer = pd.ExcelWriter(file_name_t2 + '.xlsx')\n",
    "result_df_t2.to_excel(writer,'Sheet1')\n",
    "\n",
    "# Save file\n",
    "writer.save() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class PreML:\n",
    "    \n",
    "    def __init__(self, dataframe, search1, image_name):\n",
    "        self.df = dataframe\n",
    "        self.ss = search1\n",
    "        self.img = image_name\n",
    "        \n",
    "    def select_image_type(self):\n",
    "        \"\"\" Another select image type function this time solely for selecting the image type\"\"\"\n",
    "        df_result = self.df.loc[self.df['file_name'].str.contains(self.ss, case=False)]\n",
    "        return df_result\n",
    "    \n",
    "    def select_image_type_negative(self):\n",
    "        \"\"\" Another select image type function this time solely for selecting the image type, without also taking adc\"\"\"\n",
    "        df_result = self.df.loc[self.df['file_name'].str.contains(self.ss, case=False)]\n",
    "        return df_result\n",
    "    \n",
    "    def select_image_type_multi(self, search2):\n",
    "        \"\"\" Another select image type function this time solely for selecting the image type\"\"\"\n",
    "        df_result = self.df[(self.df['file_name'].str.contains(self.ss, case=False)) & (self.df['file_name'].str.contains(search2, case=False))]\n",
    "        return df_result\n",
    "    \n",
    "    def select_image_type_multi_negative(self, search2):\n",
    "        \"\"\" Another select image type function this time solely for selecting the image type, without also taking adc\"\"\"\n",
    "        df_result = self.df[(self.df['file_name'].str.contains(self.ss, case=False)) & (self.df['file_name'].str.contains(search2, case=False))]\n",
    "        return df_result \n",
    "    \n",
    "    def rename_clm(self, selected_df):\n",
    "        \"\"\"Function that renames the column names to be more specific, i.e. image type gets added\"\"\"\n",
    "        lst_clm = selected_df.columns.tolist()\n",
    "        df_copy = selected_df.loc[:]\n",
    "        skip_list = ['Anon_ID',  'SeriesDescription', 'B-Value', 'DCM_header_path'] \n",
    "\n",
    "        for clm in lst_clm:\n",
    "            if clm not in skip_list:\n",
    "                new_name =  clm + \"-\" + self.img\n",
    "                df_copy.rename(columns={clm:new_name}, inplace=True)\n",
    "        return df_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_2d = ['int1_t2_0_5_2D','int1_t2_0_35_2D','int1_dwi_1_37_2D','int1_dwi_2_2D',\n",
    "        'int2_t2_0_5_2D','int2_t2_0_35_2D','int2_dwi_1_37_2D','int2_dwi_2_2D',\n",
    "        'int3_t2_0_5_2D','int3_t2_0_35_2D','int3_dwi_1_37_2D','int3_dwi_2_2D',\n",
    "        'int10_t2_0_5_2D','int10_t2_0_35_2D','int10_dwi_1_37_2D','int10_dwi_2_2D',\n",
    "        'int1_t2_0_8_2D','int1_dwi_2_5_2D', 'int2_t2_0_8_2D','int2_dwi_2_5_2D',\n",
    "        'int3_t2_0_8_2D','int3_dwi_2_5_2D', 'int10_t2_0_8_2D','int10_dwi_2_5_2D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['int1_t2_0_5','int1_t2_0_35','int1_dwi_1_37','int1_dwi_2',\n",
    "        'int2_t2_0_5','int2_t2_0_35','int2_dwi_1_37','int2_dwi_2',\n",
    "        'int3_t2_0_5','int3_t2_0_35','int3_dwi_1_37','int3_dwi_2',\n",
    "        'int10_t2_0_5','int10_t2_0_35','int10_dwi_1_37','int10_dwi_2',\n",
    "        'int1_t2_0_8','int1_dwi_2_5', 'int2_t2_0_8','int2_dwi_2_5',\n",
    "        'int3_t2_0_8','int3_dwi_2_5', 'int10_t2_0_8','int10_dwi_2_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['int1_dwi_1_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    print(name)\n",
    "    name1 = './/non_sorted_radiomics/resampled/3D/[Radiomics_data]_20mmDLM_3D_'+ name +'.csv'\n",
    "\n",
    "    # Import the calculated features in the form of a csv file with pandas\n",
    "    test_features = pd.read_csv(name1, error_bad_lines=False, sep=',')\n",
    "\n",
    "    # Create copy to work in\n",
    "    test_data = test_features.loc[:]\n",
    "    test_data['file_name'] = test_data.file_name.str.replace('\\\\','=')\n",
    "    del test_data[\"diagnostics_Image-original_Dimensionality\"]\n",
    "    del test_data['Unnamed: 0']\n",
    "    test_data = test_data.dropna()\n",
    "    print(test_data.shape)\n",
    "    if \"_t2_\" in name:\n",
    "        # T2\n",
    "        t2_search1 = \"t2\"\n",
    "        t2_sag_search2 = \"sag\"\n",
    "        T2Sag = PreML(test_data, t2_search1, \"T2-sag\")\n",
    "\n",
    "        # execute\n",
    "        sag_selected = T2Sag.select_image_type_multi(t2_sag_search2)\n",
    "        t2_sag = T2Sag.rename_clm(sag_selected)\n",
    "        t2_sag_dd = t2_sag.drop_duplicates(subset=['Anon_ID'])\n",
    "        \n",
    "        # T2 cor\n",
    "        t2_cor_search2 = \"cor\"\n",
    "        T2Cor = PreML(test_data, t2_search1, \"T2-cor\")\n",
    "\n",
    "        # execute\n",
    "        cor_selected = T2Cor.select_image_type_multi(t2_cor_search2)\n",
    "        t2_cor = T2Cor.rename_clm(cor_selected)\n",
    "        t2_cor_dd = t2_cor.drop_duplicates(subset=['Anon_ID'])\n",
    "        \n",
    "        # T2 tra\n",
    "        t2_tra_search2 = \"tra\"\n",
    "        T2Tra = PreML(test_data, t2_search1, \"T2-tra\")\n",
    "\n",
    "        # execute\n",
    "        tra_selected = T2Tra.select_image_type_multi(t2_tra_search2)\n",
    "        t2_tra = T2Tra.rename_clm(tra_selected)\n",
    "        t2_tra_dd = t2_tra.drop_duplicates(subset=['Anon_ID'])      \n",
    "\n",
    "        # Merge on anon identifiers      \n",
    "        df_1 = t2_tra_dd.merge(t2_cor_dd,on='Anon_ID')\n",
    "        df_2 = df_1.merge(t2_sag_dd,on='Anon_ID')\n",
    "        \n",
    "        name2 = 'SRDart4_20mmDLM_' + name + '.csv'\n",
    "        # save as csv, change name\n",
    "        name3 = name2.replace('[','')\n",
    "        name4 = name3.replace(']','')\n",
    "        df_2.to_csv(name4)\n",
    "        print(df_2.shape)\n",
    "        \n",
    "    else:\n",
    "\n",
    "        # b50\n",
    "        b50_search1 = \"=b-0=|=b-50=|=b-100=\"\n",
    "        B50 = PreML(test_data, b50_search1, \"DWI-b50\")\n",
    "\n",
    "        # execute\n",
    "        b50_selected = B50.select_image_type_negative()\n",
    "        b50 = B50.rename_clm(b50_selected)\n",
    "        b50_dd = b50.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "        # b400/500\n",
    "        b400_search1 = \"=b-400=|=b-500=\"\n",
    "        B400 = PreML(test_data, b400_search1, \"DWI-b400\")\n",
    "\n",
    "        # execute\n",
    "        b400_selected = B400.select_image_type_negative()\n",
    "        b400 = B400.rename_clm(b400_selected)\n",
    "        b400_dd = b400.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "        # b800/100/750\n",
    "        b800_search1 = \"=b-800=|=b-1000=|=b-750=\"\n",
    "        B800 = PreML(test_data, b800_search1, \"DWI-b800\")\n",
    "\n",
    "        # execute\n",
    "        b800_selected = B800.select_image_type_negative()\n",
    "        b800 = B800.rename_clm(b800_selected)\n",
    "        b800_dd = b800.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "        # bcal\n",
    "        b1400_search1 = \"=b-2000=|=b-1400=|=b-1500=\"\n",
    "        B1400 = PreML(test_data, b1400_search1, \"DWI-b1400\")\n",
    "\n",
    "        # execute\n",
    "        b1400_selected = B1400.select_image_type_negative()\n",
    "        b1400 = B1400.rename_clm(b1400_selected)\n",
    "        b1400_dd = b1400.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "        # adc\n",
    "        adc_search1 = \"adc\"\n",
    "        ADC = PreML(test_data, adc_search1, \"ADC\")\n",
    "\n",
    "        # execute\n",
    "        adc_selected = ADC.select_image_type_negative()\n",
    "        adc = ADC.rename_clm(adc_selected)\n",
    "        adc_dd = adc.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "         # Merge results back together\n",
    "        df_1 = b50_dd.merge(b400_dd,on='Anon_ID')\n",
    "        df_2 = df_1.merge(b800_dd,on='Anon_ID')\n",
    "        df_3 = df_2.merge(b1400_dd,on='Anon_ID')\n",
    "        df_4 = df_3.merge(adc_dd,on='Anon_ID')\n",
    "\n",
    "        name2 = 'SRDart4_20mmDLM_' + name + '.csv'\n",
    "        name3 = name2.replace('[','')\n",
    "        name4 = name3.replace(']','')\n",
    "        # save as csv, change name\n",
    "        df_4.to_csv(name4)\n",
    "        print(df_4.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# no resampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./non_sorted_radiomics/noresampling/3D/[Radiomics_data]_20mmDLM_3D_noresampling.csv\"\n",
    "\n",
    "# Import the calculated features in the form of a csv file with pandas\n",
    "test_features = pd.read_csv(file_path, error_bad_lines=False, sep=',')\n",
    "\n",
    "# Create copy to work in\n",
    "test_data = test_features.loc[:]\n",
    "test_data['file_name'] = test_data.file_name.str.replace('\\\\','=')\n",
    "del test_data[\"diagnostics_Image-original_Dimensionality\"]\n",
    "del test_data['Unnamed: 0']\n",
    "test_data = test_data.dropna()\n",
    "print(test_data.shape)\n",
    "\n",
    "# T2\n",
    "t2_search1 = \"t2\"\n",
    "t2_sag_search2 = \"sag\"\n",
    "T2Sag = PreML(test_data, t2_search1, \"T2-sag\")\n",
    "\n",
    "# execute\n",
    "sag_selected = T2Sag.select_image_type_multi(t2_sag_search2)\n",
    "t2_sag = T2Sag.rename_clm(sag_selected)\n",
    "t2_sag_dd = t2_sag.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "# T2 cor\n",
    "t2_cor_search2 = \"cor\"\n",
    "T2Cor = PreML(test_data, t2_search1, \"T2-cor\")\n",
    "\n",
    "# execute\n",
    "cor_selected = T2Cor.select_image_type_multi(t2_cor_search2)\n",
    "t2_cor = T2Cor.rename_clm(cor_selected)\n",
    "t2_cor_dd = t2_cor.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "# T2 tra\n",
    "t2_tra_search2 = \"tra\"\n",
    "T2Tra = PreML(test_data, t2_search1, \"T2-tra\")\n",
    "\n",
    "# execute\n",
    "tra_selected = T2Tra.select_image_type_multi(t2_tra_search2)\n",
    "t2_tra = T2Tra.rename_clm(tra_selected)\n",
    "t2_tra_dd = t2_tra.drop_duplicates(subset=['Anon_ID'])      \n",
    "\n",
    "# b50\n",
    "b50_search1 = \"=b-0=|=b-50=|=b-100=\"\n",
    "B50 = PreML(test_data, b50_search1, \"DWI-b50\")\n",
    "\n",
    "# execute\n",
    "b50_selected = B50.select_image_type_negative()\n",
    "b50 = B50.rename_clm(b50_selected)\n",
    "b50_dd = b50.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "# b400/500\n",
    "b400_search1 = \"=b-400=|=b-500=\"\n",
    "B400 = PreML(test_data, b400_search1, \"DWI-b400\")\n",
    "\n",
    "# execute\n",
    "b400_selected = B400.select_image_type_negative()\n",
    "b400 = B400.rename_clm(b400_selected)\n",
    "b400_dd = b400.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "# b800/100/750\n",
    "b800_search1 = \"=b-800=|=b-1000=|=b-750=\"\n",
    "B800 = PreML(test_data, b800_search1, \"DWI-b800\")\n",
    "\n",
    "# execute\n",
    "b800_selected = B800.select_image_type_negative()\n",
    "b800 = B800.rename_clm(b800_selected)\n",
    "b800_dd = b800.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "# bcal\n",
    "b1400_search1 = \"=b-2000=|=b-1400=|=b-1500=\"\n",
    "B1400 = PreML(test_data, b1400_search1, \"DWI-b1400\")\n",
    "\n",
    "# execute\n",
    "b1400_selected = B1400.select_image_type_negative()\n",
    "b1400 = B1400.rename_clm(b1400_selected)\n",
    "b1400_dd = b1400.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    "# adc\n",
    "adc_search1 = \"adc\"\n",
    "ADC = PreML(test_data, adc_search1, \"ADC\")\n",
    "\n",
    "# execute\n",
    "adc_selected = ADC.select_image_type_negative()\n",
    "adc = ADC.rename_clm(adc_selected)\n",
    "adc_dd = adc.drop_duplicates(subset=['Anon_ID'])\n",
    "\n",
    " # Merge results back together\n",
    "df_1 = t2_sag_dd.merge(t2_cor_dd,on='Anon_ID')\n",
    "df_2 = df_1.merge(t2_tra_dd,on='Anon_ID')\n",
    "df_3 = df_2.merge(b50_dd,on='Anon_ID')\n",
    "df_4 = df_3.merge(b400_dd,on='Anon_ID')\n",
    "df_5 = df_4.merge(b800_dd,on='Anon_ID')\n",
    "df_6 = df_5.merge(b1400_dd,on='Anon_ID')\n",
    "df_7 = df_6.merge(adc_dd,on='Anon_ID')\n",
    "print(df_7.shape)\n",
    "\n",
    "df_7.to_csv(\"SRDart4_20mmDLM_3D_noresampling.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_path_seeker(dir_path, int_set):\n",
    "    df_merge = pd.read_csv(\"./non_sorted_radiomics/resampled/2D/[Radiomics_data]_20mmDLM_2D_int1_t2_0_5_2D.csv\", index_col=0)\n",
    "    \"\"\"Function that that uses the dir_path found with the other function to grab a csv path\"\"\"\n",
    "    for dir_path, sub_dir, files in os.walk(os.path.normpath(dir_path), topdown=True):\n",
    "        for file_ in files:\n",
    "            if file_.endswith(\".csv\"):\n",
    "                full_path = os.path.join(dir_path,file_)\n",
    "                if int_set in full_path:\n",
    "                    csv_temp = pd.read_csv(full_path, index_col=0)\n",
    "                    \n",
    "    return df_list                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['int1_t2_0_5_2D','int1_t2_0_35_2D','int1_dwi_1_37_2D','int1_dwi_2_2D',\n",
    "        'int2_t2_0_5_2D','int2_t2_0_35_2D','int2_dwi_1_37_2D','int2_dwi_2_2D',\n",
    "        'int3_t2_0_5_2D','int3_t2_0_35_2D','int3_dwi_1_37_2D','int3_dwi_2_2D',\n",
    "        'int10_t2_0_5_2D','int10_t2_0_35_2D','int10_dwi_1_37_2D','int10_dwi_2_2D',\n",
    "        'int1_t2_0_8_2D','int1_dwi_2_5_2D', 'int2_t2_0_8_2D','int2_dwi_2_5_2D',\n",
    "        'int3_t2_0_8_2D','int3_dwi_2_5_2D', 'int10_t2_0_8_2D','int10_dwi_2_5_2D',\n",
    "         'int1_t2_0_5','int1_t2_0_35','int1_dwi_2',\n",
    "        'int2_t2_0_5','int2_t2_0_35','int2_dwi_1_37','int2_dwi_2',\n",
    "        'int3_t2_0_5','int3_t2_0_35','int3_dwi_1_37','int3_dwi_2',\n",
    "        'int10_t2_0_5','int10_t2_0_35','int10_dwi_1_37','int10_dwi_2',\n",
    "        'int1_t2_0_8','int1_dwi_2_5', 'int2_t2_0_8','int2_dwi_2_5',\n",
    "        'int3_t2_0_8','int3_dwi_2_5', 'int10_t2_0_8','int10_dwi_2_5']\n",
    "\n",
    "total_names = []\n",
    "for name in names:\n",
    "    if \"2D\" in name:\n",
    "        name = \"./sorted_radiomics/resampled/2D/SRDart4_20mmDLM_\" + name + \".csv\"\n",
    "        total_names.append(name)\n",
    "    else:\n",
    "        name = \"./sorted_radiomics/resampled/3D/SRDart4_20mmDLM_\" + name + \".csv\"\n",
    "        total_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for filename in total_names:\n",
    "    df_temp = pd.read_csv(filename, index_col=0)\n",
    "    df_app = df_temp.Anon_ID\n",
    "    dfs.append(df_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "df = reduce(lambda df1,df2: pd.merge(df1,df2,on='Anon_ID'), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Total_ANON_ID_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_uf =  pd.read_csv(\"./sorted_radiomics/resampled/2D/SRDart4_20mmDLM_int2_t2_0_8_2D.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = pd.read_csv(\"./Total_ANON_ID_list.csv\", index_col=0)\n",
    "all_data = all_data_uf[all_data_uf.Anon_ID.isin(filter_df.Anon_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather MRI settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRSettings:\n",
    "    def __init__(self, information_dictionary):\n",
    "        self.dict = information_dictionary\n",
    "        \n",
    "    def dicom_path_list(self, current_path):\n",
    "        \"\"\"\n",
    "        Due to use of nifti files some header info is lost, still this is easily obtained by searching for a dicom.\n",
    "        \"\"\"\n",
    "        dicom_list = []\n",
    "\n",
    "        for dir_path, sub_dir, files in os.walk(os.path.normpath(current_path), topdown=True):\n",
    "            for file_ in files:\n",
    "                if file_.endswith(\".dcm\"):\n",
    "                    full_path = os.path.join(dir_path,file_)\n",
    "                    dicom_list.append(full_path)\n",
    "        return dicom_list\n",
    "    \n",
    "    def dicom_load(self, dicom_path):\n",
    "        # load dicom in pydicom\n",
    "        dm = pydicom.dcmread(dicom_path)  \n",
    "        \n",
    "        rt = float(dm.RepetitionTime)\n",
    "        et = float(dm.EchoTime)\n",
    "        flip = float(dm.FlipAngle)\n",
    "        sar = float(dm.SAR)\n",
    "        dbdt = float(dm.dBdt)\n",
    "        phase = float(dm[0x0018, 0x0089].value)\n",
    "        echotrain = float(dm.EchoTrainLength)\n",
    "        psamp = float(dm.PercentSampling)\n",
    "        phasefov = float(dm[0x0018, 0x0094].value)\n",
    "        PixBW = float(dm.PixelBandwidth)\n",
    "        aqmat = dm.AcquisitionMatrix\n",
    "        aqmat2 = [ float(x) for x in aqmat ]\n",
    "        aqm0 = aqmat2[0]\n",
    "        aqm1 = aqmat2[1]\n",
    "        aqm2 = aqmat2[2]\n",
    "        aqm3 = aqmat2[3]\n",
    "        nex = float(dm[0x0018, 0x0083].value)\n",
    "        sbs = float(dm.SpacingBetweenSlices)\n",
    "        direct = str(dm[0x0018, 0x1312].value)\n",
    "        pixels = dm.PixelSpacing\n",
    "        x_pix = pixels[0]\n",
    "        return rt, et, flip, sar, dbdt, phase, echotrain, psamp, phasefov, PixBW, aqm0,aqm1,aqm2,aqm3, nex, sbs, direct, x_pix \n",
    "    \n",
    "    def dicom_model(self, dicom_path):\n",
    "        # load dicom in pydicom\n",
    "        dm = pydicom.dcmread(dicom_path)         \n",
    "        model = str(dm.ManufacturerModelName)\n",
    "        mag = float(dm.MagneticFieldStrength)\n",
    "        return model, mag      \n",
    "         \n",
    "    \n",
    "    def series_cleaner(self, series_name): \n",
    "         # clean up the series descriptions\n",
    "        if 't2' in series_name.lower():\n",
    "            if 'sag' in series_name.lower():\n",
    "                series_final = \"T2-sag\"\n",
    "            if 'cor' in series_name.lower():\n",
    "                series_final = \"T2-cor\"\n",
    "            if 'tra' in series_name.lower():\n",
    "                series_final = \"T2-tra\"\n",
    "        elif 'adc' in series_name.lower() and 'b-' not in series_name:\n",
    "            series_final = \"ADC\"\n",
    "        else:\n",
    "            series_final = \"DWI-\" + series_name.replace(\"-\",\"\")\n",
    "        return series_final                           \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    def dicom_dict(self):\n",
    "        dicom_set = defaultdict(list)\n",
    "\n",
    "        # Dictionary keys are used as identifiers and can also be used to find the correct masks\n",
    "        for keys in self.dict.keys():\n",
    "            if \"mark\" in keys:\n",
    "                continue\n",
    "            for mr_and_mask in self.dict[keys]:\n",
    "                mr_file_path = mr_and_mask[0]\n",
    "\n",
    "                # Remove nifti name and grab the index for the folder path and series description\n",
    "                index1 = mr_file_path.rfind(\"\\\\\")\n",
    "\n",
    "                # Take folder path and the relevant dicom paths for each key\n",
    "                folder_path = mr_file_path[:index1]\n",
    "                dicom_path_list = self.dicom_path_list(folder_path)                \n",
    "                if \"adc\" in mr_file_path.lower() or \"t2\" in mr_file_path.lower():\n",
    "                    dicom_set[keys].append(dicom_path_list[0])\n",
    "                    \n",
    "        return dicom_set\n",
    "                \n",
    "    def dataframe_creator_ADC(self, dicoms):\n",
    "        df_list = []\n",
    "        for keys in dicoms.keys():            \n",
    "            for dicom_path in dicoms[keys]:\n",
    "                \n",
    "                # take series info\n",
    "                index1 = dicom_path.rfind(\"\\\\\")\n",
    "                index2 = dicom_path[:index1].rfind(\"\\\\\")\n",
    "                \n",
    "                # Create clean series name and dicom path list\n",
    "                series_name = dicom_path[index2+1:index1]\n",
    "                series_final = self.series_cleaner(series_name) \n",
    "                \n",
    "                if series_final == \"ADC\":\n",
    "                    rt, et, flip, sar, dbdt, phase, echotrain, psamp, phasefov, PixBW, aqm0,aqm1,aqm2,aqm3, nex, sbs,direct, x_pix = self.dicom_load(dicom_path)\n",
    "                    if direct == \"ROW\":\n",
    "                        fov = (aqm1 * x_pix)*0.1\n",
    "                    elif direct == \"COL\":\n",
    "                        fov = (aqm0 * x_pix)*0.1\n",
    "                                  \n",
    "                    \n",
    "                    temp_dict = {\"RepetitionTime-\"+series_final: rt,\"EchoTime-\"+series_final: et,\n",
    "                                \"SAR-\"+series_final: sar,\"dbdt-\"+series_final:dbdt,\n",
    "                                \"NumPhaseSteps-\"+series_final:phase, \"EchoTrainLenght-\"+series_final:echotrain, \n",
    "                                 \"PercentSampling-\"+series_final:psamp, \"PercentPhasefov-\"+series_final:phasefov,\n",
    "                                 \"PixelBandwidth-\"+series_final:PixBW,\"Matrix_0-\"+series_final:aqm0,\n",
    "                                 \"Matrix_1-\"+series_final:aqm1,\"Matrix_2-\"+series_final:aqm2,\"Matrix_3-\"+series_final:aqm3,\n",
    "                                \"NEX-\"+series_final:nex,\"SpacingBetweenSlices-\"+series_final:sbs, \"PhaseDirection-\"+series_final:direct,\n",
    "                                 \"FieldOfFiew-\"+series_final:fov}\n",
    "                    temp_df = pd.DataFrame(temp_dict, columns=temp_dict.keys(), index=[keys])\n",
    "                    \n",
    "                    model, mag = self.dicom_model(dicom_path)\n",
    "                    temp_df['Model'] = model\n",
    "                    temp_df['MagneticField'] = mag\n",
    "                    temp_df['Anon_ID'] = keys                    \n",
    "                    df_list.append(temp_df)\n",
    "        df_result = pd.concat(df_list,axis=0)\n",
    "        return df_result\n",
    "\n",
    "    def dataframe_creator_t2c(self, dicoms):\n",
    "        df_list = []\n",
    "        for keys in dicoms.keys():\n",
    "            for dicom_path in dicoms[keys]:\n",
    "                # take series info\n",
    "                index1 = dicom_path.rfind(\"\\\\\")\n",
    "                index2 = dicom_path[:index1].rfind(\"\\\\\")\n",
    "                \n",
    "                # Create clean series name and dicom path list\n",
    "                series_name = dicom_path[index2+1:index1]\n",
    "                series_final = self.series_cleaner(series_name) \n",
    "                if series_final == \"T2-cor\":\n",
    "                    rt, et, flip, sar, dbdt, phase, echotrain, psamp, phasefov, PixBW, aqm0,aqm1,aqm2,aqm3, nex, sbs,direct,x_pix= self.dicom_load(dicom_path)\n",
    "                    if direct == \"ROW\":\n",
    "                        fov = (aqm1 * x_pix)*0.1\n",
    "                    elif direct == \"COL\":\n",
    "                        fov = (aqm0 * x_pix)*0.1\n",
    "                                  \n",
    "                    temp_dict = {\"RepetitionTime-\"+series_final: rt,\"EchoTime-\"+series_final: et,\n",
    "                                \"FlipAngle-\"+series_final: flip,\"SAR-\"+series_final: sar,\"dbdt-\"+series_final:dbdt,\n",
    "                                \"NumPhaseSteps-\"+series_final:phase, \"EchoTrainLenght-\"+series_final:echotrain, \n",
    "                                 \"PercentSampling-\"+series_final:psamp, \"PercentPhasefov-\"+series_final:phasefov,\n",
    "                                 \"PixelBandwidth-\"+series_final:PixBW,\"Matrix_0-\"+series_final:aqm0,\n",
    "                                 \"Matrix_1-\"+series_final:aqm1,\"Matrix_2-\"+series_final:aqm2,\"Matrix_3-\"+series_final:aqm3,\n",
    "                                \"NEX-\"+series_final:nex,\"SpacingBetweenSlices-\"+series_final:sbs,\"PhaseDirection-\"+series_final:direct,\n",
    "                                 \"FieldOfFiew-\"+series_final:fov}\n",
    "                    temp_df = pd.DataFrame(temp_dict, columns=temp_dict.keys(), index=[keys])\n",
    "                    df_list.append(temp_df)\n",
    "                    \n",
    "        df_result = pd.concat(df_list,axis=0)\n",
    "        return df_result\n",
    "\n",
    "    def dataframe_creator_t2s(self, dicoms):\n",
    "        df_list = []\n",
    "        for keys in dicoms.keys():\n",
    "            for dicom_path in dicoms[keys]:\n",
    "                # take series info\n",
    "                index1 = dicom_path.rfind(\"\\\\\")\n",
    "                index2 = dicom_path[:index1].rfind(\"\\\\\")\n",
    "                \n",
    "                # Create clean series name and dicom path list\n",
    "                series_name = dicom_path[index2+1:index1]\n",
    "                series_final = self.series_cleaner(series_name) \n",
    "                if series_final == \"T2-sag\":\n",
    "                    rt, et, flip, sar, dbdt, phase, echotrain, psamp, phasefov, PixBW, aqm0,aqm1,aqm2,aqm3, nex, sbs,direct,x_pix = self.dicom_load(dicom_path)\n",
    "                    if direct == \"ROW\":\n",
    "                        fov = (aqm1 * x_pix)*0.1\n",
    "                    elif direct == \"COL\":\n",
    "                        fov = (aqm0 * x_pix)*0.1\n",
    "                                  \n",
    "                    temp_dict = {\"RepetitionTime-\"+series_final: rt,\"EchoTime-\"+series_final: et,\n",
    "                                \"FlipAngle-\"+series_final: flip,\"SAR-\"+series_final: sar,\"dbdt-\"+series_final:dbdt,\n",
    "                                \"NumPhaseSteps-\"+series_final:phase, \"EchoTrainLenght-\"+series_final:echotrain, \n",
    "                                 \"PercentSampling-\"+series_final:psamp, \"PercentPhasefov-\"+series_final:phasefov,\n",
    "                                 \"PixelBandwidth-\"+series_final:PixBW,\"Matrix_0-\"+series_final:aqm0,\n",
    "                                 \"Matrix_1-\"+series_final:aqm1,\"Matrix_2-\"+series_final:aqm2,\"Matrix_3-\"+series_final:aqm3,\n",
    "                                \"NEX-\"+series_final:nex,\"SpacingBetweenSlices-\"+series_final:sbs,\"PhaseDirection-\"+series_final:direct,\n",
    "                                  \"FieldOfFiew-\"+series_final:fov}\n",
    "                    temp_df = pd.DataFrame(temp_dict, columns=temp_dict.keys(), index=[keys])\n",
    "                    df_list.append(temp_df)\n",
    "                    \n",
    "        df_result = pd.concat(df_list,axis=0)\n",
    "        return df_result\n",
    "\n",
    "    def dataframe_creator_t2t(self, dicoms):\n",
    "        df_list = []\n",
    "        for keys in dicoms.keys():\n",
    "            for dicom_path in dicoms[keys]:\n",
    "                # take series info\n",
    "                index1 = dicom_path.rfind(\"\\\\\")\n",
    "                index2 = dicom_path[:index1].rfind(\"\\\\\")\n",
    "                \n",
    "                # Create clean series name and dicom path list\n",
    "                series_name = dicom_path[index2+1:index1]\n",
    "                series_final = self.series_cleaner(series_name) \n",
    "                if series_final == \"T2-tra\":\n",
    "                    rt, et, flip, sar, dbdt, phase, echotrain, psamp, phasefov, PixBW, aqm0,aqm1,aqm2,aqm3, nex, sbs,direct,x_pix = self.dicom_load(dicom_path)\n",
    "                    if direct == \"ROW\":\n",
    "                        fov = (aqm1 * x_pix)*0.1\n",
    "                    elif direct == \"COL\":\n",
    "                        fov = (aqm0 * x_pix)*0.1\n",
    "                                  \n",
    "                    temp_dict = {\"RepetitionTime-\"+series_final: rt,\"EchoTime-\"+series_final: et,\n",
    "                                \"FlipAngle-\"+series_final: flip,\"SAR-\"+series_final: sar,\"dbdt-\"+series_final:dbdt,\n",
    "                                \"NumPhaseSteps-\"+series_final:phase, \"EchoTrainLenght-\"+series_final:echotrain, \n",
    "                                 \"PercentSampling-\"+series_final:psamp, \"PercentPhasefov-\"+series_final:phasefov,\n",
    "                                 \"PixelBandwidth-\"+series_final:PixBW,\"Matrix_0-\"+series_final:aqm0,\n",
    "                                 \"Matrix_1-\"+series_final:aqm1,\"Matrix_2-\"+series_final:aqm2,\"Matrix_3-\"+series_final:aqm3,\n",
    "                                \"NEX-\"+series_final:nex,\"SpacingBetweenSlices-\"+series_final:sbs,\"PhaseDirection-\"+series_final:direct,\n",
    "                                  \"FieldOfFiew-\"+series_final:fov}\n",
    "                    temp_df = pd.DataFrame(temp_dict, columns=temp_dict.keys(), index=[keys])\n",
    "                    df_list.append(temp_df)\n",
    "                    \n",
    "        df_result = pd.concat(df_list,axis=0)\n",
    "        return df_result\n",
    "    \n",
    "    def dataframe_combinator(self, dicoms):\n",
    "        df_adc = self.dataframe_creator_ADC(dicoms)\n",
    "        df_t2c = self.dataframe_creator_t2c(dicoms)\n",
    "        df_t2s = self.dataframe_creator_t2s(dicoms)\n",
    "        df_t2t = self.dataframe_creator_t2t(dicoms)\n",
    "        \n",
    "        return pd.concat([df_adc,df_t2c,df_t2s,df_t2t],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRset = MRSettings(mri_mask_list_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_dicoms = MRset.dicom_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_settings = MRset.dataframe_combinator(relevant_dicoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_settings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-PyRadiomics]",
   "language": "python",
   "name": "conda-env-.conda-PyRadiomics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
